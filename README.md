# March Machine Learning Mania 2026 â€” Lakehouse local (MinIO) + Spark 4 + PySpark (Docker, Windows)

This repository provides an **enterprise-style local Big Data setup** (100% free) to process Kaggle CSV files (~180MB+) using:

- **Apache Spark 4** (official Docker image)
- **MinIO** (S3-compatible **local data lake**)
- **Bronze / Silver / Gold** lakehouse layout (Parquet)
- **PySpark jobs** with reusable Spark session config + logging
- Simple **Makefile** shortcuts and a clean project structure

It is designed for:
- Kaggle competitions (like March Machine Learning Mania) where you want a **reproducible**, **cloud-like** pipeline
- Portfolio / exam / interview demonstrations of a modern data platform

---

## Architecture

**Raw CSV (local)** â†’ (PySpark ingest) â†’ **MinIO / Bronze (Parquet)**  
â†’ (features) â†’ **Silver**  
â†’ (training dataset) â†’ **Gold**  
â†’ (model training, optional) â†’ `artifacts/`

MinIO is S3-compatible, so this setup mirrors AWS S3 / GCS / ADLS patterns while running fully locally.

---

## Prerequisites (Windows)

1. **Docker Desktop** installed and running (WSL2 enabled).
2. Optional: `make` (or use the raw docker commands below).

---

## Quick start

### 1) Copy env file

```bash
copy .env.example .env
```

### 2) Start the platform (MinIO + Spark cluster)

```bash
docker compose up -d
```

This starts:
- `minio` (S3 API: http://localhost:9000, Console: http://localhost:9001)
- `minio-mc` (creates the bucket on startup)
- `spark-master` (Spark UI: http://localhost:8080)
- `spark-worker-1` (Worker UI: http://localhost:8081)

### 3) Put Kaggle CSV files into the input folder

Put your Kaggle CSVs into:

```
data/input/
```

Example:
- `data/input/MRegularSeasonCompactResults.csv`
- `data/input/MTeams.csv`
- etc.

> Note: files in `data/input/` are **mounted into the Spark container** at `/opt/project/data/input`.

### 4) Run the pipeline (Bronze â†’ Silver â†’ Gold)

Run each job with `docker compose run` (recommended) or `make`.

**Bronze (CSV â†’ Parquet in MinIO):**
```bash
docker compose run --rm spark-submit python jobs/01_ingest_bronze.py
```

**Silver (feature engineering):**
```bash
docker compose run --rm spark-submit python jobs/02_build_silver_features.py
```

**Gold (training dataset for matchups):**
```bash
docker compose run --rm spark-submit python jobs/03_build_gold_training_set.py
```

### 5) Verify data in MinIO

Open MinIO console: http://localhost:9001  
Login with values from `.env` (default `admin/admin123`)

Bucket: `kaggle-lake`  
You should see:
- `bronze/...`
- `silver/...`
- `gold/...`

---

## Where is the â€œdata lakeâ€?

It is MinIO (S3-compatible) running locally.

- S3 endpoint: `http://localhost:9000`
- Bucket: `kaggle-lake`
- Example path: `s3a://kaggle-lake/bronze/march_mania/...`

---

## Project layout

```
.
â”œâ”€ docker-compose.yml
â”œâ”€ .env.example
â”œâ”€ Makefile
â”œâ”€ jobs/                         # PySpark batch jobs
â”‚  â”œâ”€ 01_ingest_bronze.py
â”‚  â”œâ”€ 02_build_silver_features.py
â”‚  â””â”€ 03_build_gold_training_set.py
â”œâ”€ src/
â”‚  â”œâ”€ common/
â”‚  â”‚  â”œâ”€ spark.py                # SparkSession factory (S3A / MinIO)
â”‚  â”‚  â”œâ”€ paths.py                # Centralized lake paths
â”‚  â”‚  â””â”€ logging.py              # Consistent logging
â”‚  â””â”€ features/
â”‚     â””â”€ basketball_features.py  # Reusable feature builders
â”œâ”€ data/
â”‚  â””â”€ input/                     # Put Kaggle CSV here (not committed)
â””â”€ artifacts/                    # Optional exports (submissions, models)
```

---

## Notes for Spark 4

This stack uses the **official Apache Spark Docker image** (`apache/spark:4.x`) from Docker Hub.  
See Spark docker images documentation and tags:
- Apache Spark docker availability on Apache website (Downloads â†’ Installing with Docker)
- `apache/spark` tags on Docker Hub

---

## Troubleshooting

### MinIO console opens, but bucket not created
Run:
```bash
docker compose logs minio-mc
```
Then rerun:
```bash
docker compose up -d minio-mc
```

### Spark job cannot access `s3a://...`
Ensure:
- `MINIO_ROOT_USER` / `MINIO_ROOT_PASSWORD` in `.env`
- Spark job uses `src/common/spark.py` which configures S3A
- Containers are on the same docker network (compose handles that)

---

## Next steps (optional)

- Add ML model training (Spark MLlib / sklearn) using `gold` dataset.
- Add orchestration (Airflow) and data quality checks (Great Expectations).
- Add experiment tracking (MLflow).

---

## License
MIT


---

## Model training + Kaggle submission export (optional)

After running Bronze â†’ Silver â†’ Gold, you can train a baseline model and export `submission.csv`.

1) Put the Kaggle sample submission into `data/input/` (one of):
   - `MSampleSubmissionStage1.csv`
   - `MSampleSubmissionStage2.csv`
   - `sample_submission.csv`

2) Run:

```bash
docker compose run --rm spark-submit python jobs/04_train_and_export_submission.py
```

Output:
- `artifacts/submission.csv`

### Leakage note (important)
This repo uses a **season-based split** for validation (latest season as validation) to reduce time-series leakage.
For stronger setups, you can:
- use multiple seasons as validation (rolling backtests),
- remove tournament games from feature computation,
- build ELO per season with strict time ordering.


---

## ğŸ“‚ Project Structure (Important)

Put your Kaggle CSV files inside:

```
data/input/
```

Example:

```
march-mania-spark-lakehouse/
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env
â”œâ”€â”€ Makefile
â”‚
â”œâ”€â”€ conf/
â”‚   â””â”€â”€ log4j2.properties
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ input/                # â† PUT YOUR KAGGLE CSV FILES HERE
â”‚       â”œâ”€â”€ MRegularSeasonCompactResults.csv
â”‚       â”œâ”€â”€ MTeams.csv
â”‚       â”œâ”€â”€ MNCAATourneyCompactResults.csv
â”‚       â”œâ”€â”€ MSampleSubmissionStage1.csv
â”‚
â”œâ”€â”€ artifacts/
â”‚   â””â”€â”€ submission.csv        # Generated Kaggle submission file
â”‚
â”œâ”€â”€ jobs/
â”‚   â”œâ”€â”€ 01_ingest_bronze.py
â”‚   â”œâ”€â”€ 02_build_silver_features.py
â”‚   â”œâ”€â”€ 03_build_gold_training_set.py
â”‚   â””â”€â”€ 04_train_and_export_submission.py
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ common/
â”‚   â”‚   â”œâ”€â”€ spark.py
â”‚   â”‚   â”œâ”€â”€ paths.py
â”‚   â”‚   â””â”€â”€ logging.py
â”‚   â”‚
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â””â”€â”€ basketball_features.py
â”‚   â”‚
â”‚   â””â”€â”€ ml/
â”‚       â””â”€â”€ modeling.py
â”‚
â””â”€â”€ .github/
    â””â”€â”€ workflows/
        â””â”€â”€ ci.yml
```

### ğŸ— Lakehouse Logical Architecture

Bronze:
```
s3a://kaggle-lake/bronze/march_mania/
```

Silver:
```
s3a://kaggle-lake/silver/march_mania/
```

Gold:
```
s3a://kaggle-lake/gold/march_mania/
```

Data flow:

Raw CSV (data/input/)  
â†’ Spark ingestion  
â†’ Bronze (Parquet)  
â†’ Silver (features)  
â†’ Gold (ML dataset)  
â†’ Model training  
â†’ artifacts/submission.csv  


---

## ğŸ”¥ Expert Mode (Kaggle-competitive)

This repo includes an **expert pipeline** designed to improve Kaggle LogLoss scores:

- **Season-scoped ELO** (reset each season) computed from regular season games
- **Causal rolling momentum features** (last N games, excluding the current game)
- **Rolling season backtest** (train <= season-1, validate = season)
- Export:
  - `artifacts/backtest_metrics.csv`
  - `artifacts/submission_blend.csv`

### Run expert feature jobs

```bash
docker compose run --rm spark-submit python jobs/05_build_silver_elo.py
docker compose run --rm spark-submit python jobs/06_build_silver_rolling.py
```

### Run expert backtest + export

```bash
docker compose run --rm spark-submit python jobs/07_backtest_and_export_blend.py
```

### Configure experiments

Edit:

- `conf/pipeline.yml` (ELO K-factor, rolling window size, model parameters, backtest range)
