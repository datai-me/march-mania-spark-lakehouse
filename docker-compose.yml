# March Mania — Lakehouse (MinIO + Spark 4) — stable/offline/pro
# Usage:
#   docker compose up -d
#   docker compose run --rm spark-submit jobs/run_pipeline.py --list 1,2,5,6,3,4

x-spark-volumes: &spark-volumes
  volumes:
    - ./jobs:/opt/project/jobs:ro
    - ./src:/opt/project/src:ro
    - ./src.zip:/opt/project/src.zip:ro   # ✅ executors-friendly
    - ./conf:/opt/project/conf:ro
    - ./data:/opt/project/data
    - ./artifacts:/opt/project/artifacts

x-spark-build: &spark-build
  build:
    context: .
    dockerfile: Dockerfile.spark

x-spark-network: &spark-network
  networks:
    - lakehouse

services:
  minio:
    image: minio/minio:latest
    container_name: minio
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-admin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-admin123}
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "sh", "-lc", "wget -qO- http://localhost:9000/minio/health/ready >/dev/null 2>&1"]
      interval: 5s
      timeout: 3s
      retries: 20
    <<: *spark-network

  minio-init:
    image: minio/mc:latest
    container_name: minio-init
    depends_on:
      minio:
        condition: service_healthy
    <<: *spark-network
    entrypoint: >
      /bin/sh -c "
        mc alias set local http://minio:9000 ${MINIO_ROOT_USER:-admin} ${MINIO_ROOT_PASSWORD:-admin123} &&
        mc mb --ignore-existing local/${MINIO_BUCKET:-kaggle-lake} &&
        mc anonymous set none local/${MINIO_BUCKET:-kaggle-lake} &&
        echo '✅ Bucket ${MINIO_BUCKET:-kaggle-lake} prêt.'
      "

  spark-master:
    <<: [*spark-build, *spark-volumes, *spark-network]
    container_name: spark-master
    environment:
      SPARK_MODE: master
      SPARK_MASTER_HOST: spark-master
      TZ: ${TZ:-Indian/Antananarivo}
    command:
      - /opt/spark/bin/spark-class
      - org.apache.spark.deploy.master.Master
      - --host
      - spark-master
      - --port
      - "7077"
      - --webui-port
      - "8080"
    ports:
      - "7077:7077"
      - "8080:8080"

  spark-worker-1:
    <<: [*spark-build, *spark-volumes, *spark-network]
    container_name: spark-worker-1
    depends_on:
      - spark-master
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      TZ: ${TZ:-Indian/Antananarivo}
    command:
      - /opt/spark/bin/spark-class
      - org.apache.spark.deploy.worker.Worker
      - spark://spark-master:7077
      - --webui-port
      - "8081"
    ports:
      - "8081:8081"

  # Runner (spark-submit)
  # IMPORTANT:
  #  - Only src.zip is shipped to executors via --py-files
  #  - run_pipeline.py MUST call spark-submit with --py-files too (as we fixed earlier)
  spark-submit:
    <<: [*spark-build, *spark-volumes, *spark-network]
    container_name: spark-submit
    depends_on:
      - spark-master
      - minio-init
    environment:
      TZ: ${TZ:-Indian/Antananarivo}

      # for run_pipeline.py (so it reuses same endpoints)
      SPARK_MASTER_URL: spark://spark-master:7077
      MINIO_ENDPOINT: http://minio:9000
      MINIO_ACCESS_KEY: ${MINIO_ROOT_USER:-admin}
      MINIO_SECRET_KEY: ${MINIO_ROOT_PASSWORD:-admin123}
      MINIO_BUCKET: ${MINIO_BUCKET:-kaggle-lake}

      # driver path (nice-to-have)
      PYTHONPATH: /opt/project

    working_dir: /opt/project
    entrypoint: ["/opt/spark/bin/spark-submit"]
    command:
      - --master
      - spark://spark-master:7077

      # Ship project code to executors
      - --py-files
      - /opt/project/src.zip

      # S3A → MinIO
      - --conf
      - spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
      - --conf
      - spark.hadoop.fs.s3a.path.style.access=true
      - --conf
      - spark.hadoop.fs.s3a.endpoint=http://minio:9000
      - --conf
      - spark.hadoop.fs.s3a.access.key=${MINIO_ROOT_USER:-admin}
      - --conf
      - spark.hadoop.fs.s3a.secret.key=${MINIO_ROOT_PASSWORD:-admin123}
      - --conf
      - spark.hadoop.fs.s3a.connection.ssl.enabled=false

      # Python
      - --conf
      - spark.pyspark.python=python
      - --conf
      - spark.pyspark.driver.python=python
      - --conf
      - spark.executorEnv.PYTHONPATH=/opt/project/src.zip:/opt/project

      # Logging
      - --conf
      - spark.driver.extraJavaOptions=-Dlog4j.configurationFile=/opt/project/conf/log4j2.properties
      - --conf
      - spark.executor.extraJavaOptions=-Dlog4j.configurationFile=/opt/project/conf/log4j2.properties

networks:
  lakehouse:
    driver: bridge

volumes:
  minio_data:
    driver: local