"""
Pipeline Runner ‚Äî Ex√©cute les jobs Spark en mode Docker OU local (dual-mode).

D√©tection automatique du runtime :
  - Docker  : SPARK_HOME=/opt/spark (dans le conteneur spark-submit)
  - Local   : SPARK_HOME d√©fini dans l'environnement, ou spark-submit dans le PATH

Usage :
    python jobs/run_pipeline.py             # liste les jobs disponibles
    python jobs/run_pipeline.py 1 10        # jobs 01 √† 10
    python jobs/run_pipeline.py 3           # job 03 uniquement
    python jobs/run_pipeline.py 1 3 7 12    # jobs 01, 03, 07, 12 (liste libre)

Variables d'environnement :
    SPARK_MASTER    url du master (d√©faut : spark://spark-master:7077 en Docker,
                    local[*] en mode local)
    PIPELINE_MODE   forcer "docker" ou "local" (optionnel)
"""

import os
import sys
import time
import subprocess
from pathlib import Path

JOBS_DIR = Path(__file__).parent

# ‚îÄ‚îÄ D√©tection automatique du runtime ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def detect_mode() -> str:
    """
    Retourne 'docker' si on tourne dans le conteneur spark-submit,
    'local' sinon. Peut √™tre forc√© via la variable PIPELINE_MODE.
    """
    forced = os.environ.get("PIPELINE_MODE", "").lower()
    if forced in {"docker", "local"}:
        return forced
    # Dans le conteneur, SPARK_HOME est fix√© √† /opt/spark
    return "docker" if os.environ.get("SPARK_HOME") == "/opt/spark" else "local"


def get_spark_submit_cmd(mode: str) -> list[str]:
    """
    Retourne la commande spark-submit adapt√©e au mode.
    - Docker : chemin absolu dans le conteneur
    - Local  : cherche spark-submit dans SPARK_HOME ou dans le PATH
    """
    if mode == "docker":
        return ["/opt/spark/bin/spark-submit", "--master", "spark://spark-master:7077"]

    # Mode local : utilise SPARK_HOME si d√©fini, sinon cherche dans le PATH
    spark_home = os.environ.get("SPARK_HOME")
    spark_submit = str(Path(spark_home) / "bin" / "spark-submit") if spark_home else "spark-submit"
    master = os.environ.get("SPARK_MASTER", "local[*]")
    return [spark_submit, "--master", master]


# ‚îÄ‚îÄ D√©couverte des jobs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def discover_jobs() -> dict[int, str]:
    """Retourne {num√©ro: nom_fichier} tri√©, en excluant les scripts utilitaires."""
    jobs = {}
    for file in JOBS_DIR.glob("*.py"):
        if file.stem.startswith("run_"):
            continue
        prefix = file.name.split("_", 1)[0]
        if prefix.isdigit():
            jobs[int(prefix)] = file.name
    return dict(sorted(jobs.items()))


# ‚îÄ‚îÄ Ex√©cution d'un job ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def run_job(job_file: str, spark_cmd: list[str]) -> bool:
    """Lance un job et retourne True si succ√®s."""
    print(f"\n{'‚îÄ' * 60}")
    print(f"  ‚ñ∂  {job_file}")
    print(f"{'‚îÄ' * 60}")
    start = time.time()

    result = subprocess.run(
        [*spark_cmd, str(JOBS_DIR / job_file)],
        check=False,
    )

    elapsed = time.time() - start
    if result.returncode == 0:
        print(f"  ‚úÖ  {job_file} ‚Äî {elapsed:.1f}s")
        return True
    else:
        print(f"  ‚ùå  {job_file} ‚Äî √âCHEC (code {result.returncode}) apr√®s {elapsed:.1f}s")
        return False


# ‚îÄ‚îÄ Parsing des arguments CLI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def parse_args(args: list[str]) -> list[int]:
    """
    Interpr√®te les arguments :
        "3"       ‚Üí [3]
        "1 10"    ‚Üí [1, 2, ..., 10]   (plage continue si exactement 2 args)
        "1 3 7"   ‚Üí [1, 3, 7]         (liste libre si 3+ args)
    """
    if len(args) == 1:
        return [int(args[0])]
    if len(args) == 2:
        return list(range(int(args[0]), int(args[1]) + 1))
    return [int(a) for a in args]


# ‚îÄ‚îÄ Main ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def main() -> None:
    mode = detect_mode()
    spark_cmd = get_spark_submit_cmd(mode)
    jobs = discover_jobs()

    if not jobs:
        print(f"Aucun job trouv√© dans {JOBS_DIR}")
        sys.exit(1)

    args = sys.argv[1:]
    if not args:
        print(f"\nüîß  Mode d√©tect√© : {mode.upper()}")
        print(f"    spark-submit : {' '.join(spark_cmd)}\n")
        print("Jobs disponibles :\n")
        for k, v in jobs.items():
            print(f"  {k:02d}  {v}")
        print("\nUsage :")
        print("  python run_pipeline.py 1 12      # pipeline complet")
        print("  python run_pipeline.py 3         # job unique")
        print("  python run_pipeline.py 1 3 7 12  # liste libre")
        sys.exit(0)

    to_run = parse_args(args)
    print(f"\nüîß  Mode : {mode.upper()} | Jobs : {to_run}")

    total_start = time.time()
    for num in to_run:
        if num not in jobs:
            print(f"‚ö†Ô∏è  Job {num:02d} introuvable ‚Äî ignor√©.")
            continue
        if not run_job(jobs[num], spark_cmd):
            print(f"\n‚õî  Pipeline interrompu au job {num:02d}.")
            sys.exit(1)

    elapsed = time.time() - total_start
    print(f"\n{'‚ïê' * 60}")
    print(f"  üéØ  Termin√© en {elapsed:.1f}s  ({mode.upper()})")
    print(f"{'‚ïê' * 60}\n")


if __name__ == "__main__":
    main()
